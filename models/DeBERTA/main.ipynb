{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "gen = pipeline(\"token-classification\", \"lakshyakh93/deberta_finetuned_pii\", device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My name is John and I live in California.\"\n",
    "output = gen(text, aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'FIRSTNAME', 'score': 0.9546856, 'word': ' John', 'start': 10, 'end': 15}, {'entity_group': 'STATE', 'score': 0.98806274, 'word': ' California.', 'start': 29, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\Python311\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:392: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Alice Johnson, a French citizen, visited the Eiffel Tower. She works at Google and lives in Paris. \n",
    "Her email is alice.johnson@example.com. She was born on February 14, 1990, and earns $7500 per month. \n",
    "You can call her at 555-987-6543 or visit her website at www.alicejohnson.com. \n",
    "The company recently launched a new product, the Google Pixel 5, which has received positive reviews. \n",
    "In the past, Alice attended the World Cup and read War and Peace by Leo Tolstoy. \n",
    "She speaks fluent English and French. Last year, she traveled to Mount Everest in Nepal. \n",
    "The event was scheduled on July 20, 2021, at 10:00 AM. Her net worth is estimated to be $1,000,000, and she has a 25% stake in the company.\n",
    "She also has several measurements like a 5 kg bag of rice, and she finished first in the marathon.\n",
    "'''\n",
    "output = gen(text, aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'MIDDLENAME', 'score': 0.70211166, 'word': ' Johnson,', 'start': 6, 'end': 15}\n",
      "{'entity_group': 'EMAIL', 'score': 0.9976675, 'word': ' alice.johnson@example.com.', 'start': 113, 'end': 140}\n",
      "{'entity_group': 'DATE', 'score': 0.9964449, 'word': ' February 14, 1990,', 'start': 156, 'end': 175}\n",
      "{'entity_group': 'CURRENCYSYMBOL', 'score': 0.91656756, 'word': ' $7500', 'start': 185, 'end': 191}\n",
      "{'entity_group': 'PHONE_NUMBER', 'score': 0.9962837, 'word': ' 555-987-6543', 'start': 223, 'end': 236}\n",
      "{'entity_group': 'URL', 'score': 0.8163411, 'word': ' www.alicejohnson.com.', 'start': 260, 'end': 282}\n",
      "{'entity_group': 'FIRSTNAME', 'score': 0.9630687, 'word': ' Alice', 'start': 399, 'end': 405}\n",
      "{'entity_group': 'DATE', 'score': 0.9988918, 'word': ' July 20, 2021,', 'start': 585, 'end': 600}\n",
      "{'entity_group': 'TIME', 'score': 0.9992529, 'word': ' 10:00 AM.', 'start': 603, 'end': 613}\n",
      "{'entity_group': 'CURRENCYSYMBOL', 'score': 0.96038735, 'word': ' $1,000,000,', 'start': 646, 'end': 658}\n"
     ]
    }
   ],
   "source": [
    "for entity in output:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FineTuned Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice Johnson, a French citizen, visited the Eiffel Tower. She works at Google and lives in Paris.\n",
      "Her email is alice.johnson@example.com. She was born on[DATE], and earns $7500 per month.\n",
      "You can call her at 555-987-6543 or visit her website at www.alicejohnson.com.\n",
      "The company recently launched a new product, the Google Pixel 5, which has received positive reviews.\n",
      "In the past, Alice attended the World Cup and read War and Peace by Leo Tolstoy.\n",
      "She speaks fluent English and French. Last year, she traveled to Mount Everest in Nepal.\n",
      "The event was scheduled on[DATE], at[TIME]. Her net worth is estimated to be $1,000,000, and she has a 25% stake in the company.\n",
      "She also has several measurements like a 5 kg bag of rice, and she finished first in the marathon.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the fine-tuned model\n",
    "pii_detector = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "def replace_pii(text):\n",
    "    entities = pii_detector(text)\n",
    "    replaced_text = text\n",
    "    for entity in entities:\n",
    "        if entity['entity_group'] in [\"PER\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\", \"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"]:\n",
    "            replaced_text = replaced_text.replace(entity['word'], f\"[{entity['entity_group']}]\")\n",
    "    return replaced_text\n",
    "\n",
    "# Example text\n",
    "example_text = \"\"\"\n",
    "Alice Johnson, a French citizen, visited the Eiffel Tower. She works at Google and lives in Paris.\n",
    "Her email is alice.johnson@example.com. She was born on February 14, 1990, and earns $7500 per month.\n",
    "You can call her at 555-987-6543 or visit her website at www.alicejohnson.com.\n",
    "The company recently launched a new product, the Google Pixel 5, which has received positive reviews.\n",
    "In the past, Alice attended the World Cup and read War and Peace by Leo Tolstoy.\n",
    "She speaks fluent English and French. Last year, she traveled to Mount Everest in Nepal.\n",
    "The event was scheduled on July 20, 2021, at 10:00 AM. Her net worth is estimated to be $1,000,000, and she has a 25% stake in the company.\n",
    "She also has several measurements like a 5 kg bag of rice, and she finished first in the marathon.\n",
    "\"\"\"\n",
    "\n",
    "# Replace PII in the example text\n",
    "replaced_text = replace_pii(example_text)\n",
    "print(replaced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets generated and saved as 'train.csv' and 'valid.csv'.\n"
     ]
    }
   ],
   "source": [
    "# 1. Generate Synthetic Data\n",
    "\n",
    "import csv\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Generate synthetic dataset\n",
    "def generate_synthetic_data(num_samples=1000):\n",
    "    data = []\n",
    "    for _ in range(num_samples):\n",
    "        text = f\"{fake.name()} from {fake.country()} works at {fake.company()} and lives in {fake.city()}. \" \\\n",
    "               f\"They were born on {fake.date_of_birth()} and their email is {fake.email()}. \" \\\n",
    "               f\"They earn {fake.currency_symbol()}{fake.random_number(digits=5)} per year.\"\n",
    "        labels = \"O \" * len(text.split())\n",
    "        data.append((text, labels.strip()))\n",
    "    return data\n",
    "\n",
    "# Save dataset to CSV with UTF-8 encoding\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"text\", \"labels\"])\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Generate and save train and validation datasets\n",
    "train_data = generate_synthetic_data(num_samples=800)\n",
    "valid_data = generate_synthetic_data(num_samples=200)\n",
    "\n",
    "save_to_csv(train_data, 'train.csv')\n",
    "save_to_csv(valid_data, 'valid.csv')\n",
    "\n",
    "print(\"Datasets generated and saved as 'train.csv' and 'valid.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}\n",
      "{'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "for data in ner_results:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Alice Johnson, a French citizen, visited the Eiffel Tower. She works at Google and lives in Paris. \n",
    "Her email is alice.johnson@example.com. She was born on February 14, 1990, and earns $7500 per month. \n",
    "You can call her at 555-987-6543 or visit her website at www.alicejohnson.com. \n",
    "The company recently launched a new product, the Google Pixel 5, which has received positive reviews. \n",
    "In the past, Alice attended the World Cup and read War and Peace by Leo Tolstoy. \n",
    "She speaks fluent English and French. Last year, she traveled to Mount Everest in Nepal. \n",
    "The event was scheduled on July 20, 2021, at 10:00 AM. Her net worth is estimated to be $1,000,000, and she has a 25% stake in the company.\n",
    "She also has several measurements like a 5 kg bag of rice, and she finished first in the marathon.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B-PER', 'score': 0.99938476, 'index': 1, 'word': 'Alice', 'start': 1, 'end': 6}\n",
      "{'entity': 'I-PER', 'score': 0.99947315, 'index': 2, 'word': 'Johnson', 'start': 7, 'end': 14}\n",
      "{'entity': 'B-MISC', 'score': 0.9996563, 'index': 5, 'word': 'French', 'start': 18, 'end': 24}\n",
      "{'entity': 'B-LOC', 'score': 0.987009, 'index': 10, 'word': 'E', 'start': 46, 'end': 47}\n",
      "{'entity': 'I-LOC', 'score': 0.6012711, 'index': 11, 'word': '##iff', 'start': 47, 'end': 50}\n",
      "{'entity': 'B-LOC', 'score': 0.85653263, 'index': 12, 'word': '##el', 'start': 50, 'end': 52}\n",
      "{'entity': 'I-LOC', 'score': 0.9919612, 'index': 13, 'word': 'Tower', 'start': 53, 'end': 58}\n",
      "{'entity': 'B-ORG', 'score': 0.9976587, 'index': 18, 'word': 'Google', 'start': 73, 'end': 79}\n",
      "{'entity': 'B-LOC', 'score': 0.9995302, 'index': 22, 'word': 'Paris', 'start': 93, 'end': 98}\n",
      "{'entity': 'B-MISC', 'score': 0.9975721, 'index': 93, 'word': 'Google', 'start': 333, 'end': 339}\n",
      "{'entity': 'I-MISC', 'score': 0.9960896, 'index': 94, 'word': 'Pi', 'start': 340, 'end': 342}\n",
      "{'entity': 'I-MISC', 'score': 0.99602866, 'index': 95, 'word': '##xe', 'start': 342, 'end': 344}\n",
      "{'entity': 'I-MISC', 'score': 0.9973658, 'index': 96, 'word': '##l', 'start': 344, 'end': 345}\n",
      "{'entity': 'I-MISC', 'score': 0.99798536, 'index': 97, 'word': '5', 'start': 346, 'end': 347}\n",
      "{'entity': 'B-PER', 'score': 0.9977937, 'index': 109, 'word': 'Alice', 'start': 400, 'end': 405}\n",
      "{'entity': 'B-MISC', 'score': 0.998027, 'index': 112, 'word': 'World', 'start': 419, 'end': 424}\n",
      "{'entity': 'I-MISC', 'score': 0.9989466, 'index': 113, 'word': 'Cup', 'start': 425, 'end': 428}\n",
      "{'entity': 'B-MISC', 'score': 0.98951924, 'index': 116, 'word': 'War', 'start': 438, 'end': 441}\n",
      "{'entity': 'I-MISC', 'score': 0.99797946, 'index': 117, 'word': 'and', 'start': 442, 'end': 445}\n",
      "{'entity': 'I-MISC', 'score': 0.99708265, 'index': 118, 'word': 'Peace', 'start': 446, 'end': 451}\n",
      "{'entity': 'B-PER', 'score': 0.9988919, 'index': 120, 'word': 'Leo', 'start': 455, 'end': 458}\n",
      "{'entity': 'I-PER', 'score': 0.99886924, 'index': 121, 'word': 'To', 'start': 459, 'end': 461}\n",
      "{'entity': 'I-PER', 'score': 0.9729048, 'index': 122, 'word': '##ls', 'start': 461, 'end': 463}\n",
      "{'entity': 'I-PER', 'score': 0.9860728, 'index': 123, 'word': '##to', 'start': 463, 'end': 465}\n",
      "{'entity': 'I-PER', 'score': 0.46684584, 'index': 124, 'word': '##y', 'start': 465, 'end': 466}\n",
      "{'entity': 'B-MISC', 'score': 0.9993468, 'index': 129, 'word': 'English', 'start': 487, 'end': 494}\n",
      "{'entity': 'B-MISC', 'score': 0.99921656, 'index': 131, 'word': 'French', 'start': 499, 'end': 505}\n",
      "{'entity': 'B-LOC', 'score': 0.99727434, 'index': 139, 'word': 'Mount', 'start': 534, 'end': 539}\n",
      "{'entity': 'I-LOC', 'score': 0.99494296, 'index': 140, 'word': 'Everest', 'start': 540, 'end': 547}\n",
      "{'entity': 'B-LOC', 'score': 0.9997832, 'index': 142, 'word': 'Nepal', 'start': 551, 'end': 556}\n"
     ]
    }
   ],
   "source": [
    "# dslim/bert-base-NER\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = sample_text\n",
    "\n",
    "ner_results = nlp(example)\n",
    "for data in ner_results:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dslim/bert-base-NER\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = sample_text\n",
    "\n",
    "ner_results = nlp(example)\n",
    "for data in ner_results:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'MIDDLENAME', 'score': 0.70211166, 'word': ' Johnson,', 'start': 6, 'end': 15}\n",
      "{'entity_group': 'EMAIL', 'score': 0.9976675, 'word': ' alice.johnson@example.com.', 'start': 113, 'end': 140}\n",
      "{'entity_group': 'DATE', 'score': 0.9964449, 'word': ' February 14, 1990,', 'start': 156, 'end': 175}\n",
      "{'entity_group': 'CURRENCYSYMBOL', 'score': 0.91656756, 'word': ' $7500', 'start': 185, 'end': 191}\n",
      "{'entity_group': 'PHONE_NUMBER', 'score': 0.9962837, 'word': ' 555-987-6543', 'start': 223, 'end': 236}\n",
      "{'entity_group': 'URL', 'score': 0.8163411, 'word': ' www.alicejohnson.com.', 'start': 260, 'end': 282}\n",
      "{'entity_group': 'FIRSTNAME', 'score': 0.9630687, 'word': ' Alice', 'start': 399, 'end': 405}\n",
      "{'entity_group': 'DATE', 'score': 0.9988918, 'word': ' July 20, 2021,', 'start': 585, 'end': 600}\n",
      "{'entity_group': 'TIME', 'score': 0.9992529, 'word': ' 10:00 AM.', 'start': 603, 'end': 613}\n",
      "{'entity_group': 'CURRENCYSYMBOL', 'score': 0.96038735, 'word': ' $1,000,000,', 'start': 646, 'end': 658}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\Python311\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:392: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# lakshyakh93/deberta_finetuned_pii\n",
    "from transformers import pipeline\n",
    "gen = pipeline(\"token-classification\", \"lakshyakh93/deberta_finetuned_pii\", device=-1)\n",
    "\n",
    "text = sample_text\n",
    "output = gen(text, aggregation_strategy=\"first\")\n",
    "for data in output:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"ArithmeticErrorJohn Doe, born on January 1st, 1980, resides at 123 Main Street, Anytown, USA. He works at XYZ Corporation as a software engineer. His email address is john.doe@example.com and his phone number is +1 (555) 123-4567. \n",
    "\n",
    "Jane Smith, born on March 15th, 1995, lives at 456 Elm Street, Springfield, USA. She is a student at ABC University, majoring in computer science. Her email is jane.smith@example.com and her phone number is +1 (555) 987-6543.\n",
    "\n",
    "Michael Johnson, born on November 10th, 1978, lives at 789 Oak Avenue, Smalltown, USA. He works as a doctor at City Hospital. His email address is michael.johnson@example.com and his phone number is +1 (555) 222-3333.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Softwares\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\Softwares\\Python311\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'DATE', 'score': 0.9976619, 'word': ' January 1st, 1980,', 'start': 32, 'end': 51}\n",
      "{'entity_group': 'STREETADDRESS', 'score': 0.98643786, 'word': ' 123 Main Street,', 'start': 62, 'end': 79}\n",
      "{'entity_group': 'STATE', 'score': 0.8421379, 'word': ' Anytown,', 'start': 79, 'end': 88}\n",
      "{'entity_group': 'COMPANY_NAME', 'score': 0.733382, 'word': ' XYZ', 'start': 105, 'end': 109}\n",
      "{'entity_group': 'JOBTYPE', 'score': 0.5441987, 'word': ' engineer.', 'start': 135, 'end': 145}\n",
      "{'entity_group': 'EMAIL', 'score': 0.9892954, 'word': ' john.doe@example.com', 'start': 166, 'end': 187}\n",
      "{'entity_group': 'PHONE_NUMBER', 'score': 0.9046054, 'word': ' +1 (555) 123-4567.', 'start': 211, 'end': 230}\n",
      "{'entity_group': 'MIDDLENAME', 'score': 0.55161256, 'word': ' Smith,', 'start': 237, 'end': 244}\n",
      "{'entity_group': 'DATE', 'score': 0.9980107, 'word': ' March 15th, 1995,', 'start': 252, 'end': 270}\n",
      "{'entity_group': 'STREETADDRESS', 'score': 0.9916169, 'word': ' 456 Elm Street,', 'start': 279, 'end': 295}\n",
      "{'entity_group': 'CITY', 'score': 0.73172426, 'word': ' Springfield,', 'start': 295, 'end': 308}\n",
      "{'entity_group': 'EMAIL', 'score': 0.9657894, 'word': ' jane.smith@example.com', 'start': 392, 'end': 415}\n",
      "{'entity_group': 'PHONE_NUMBER', 'score': 0.93458873, 'word': ' +1 (555) 987-6543.\\n\\nMichael', 'start': 439, 'end': 467}\n",
      "{'entity_group': 'MIDDLENAME', 'score': 0.77770627, 'word': ' Johnson,', 'start': 467, 'end': 476}\n",
      "{'entity_group': 'DATE', 'score': 0.99864554, 'word': ' November 10th, 1978,', 'start': 484, 'end': 505}\n",
      "{'entity_group': 'STREETADDRESS', 'score': 0.9933029, 'word': ' 789 Oak Avenue,', 'start': 514, 'end': 530}\n",
      "{'entity_group': 'CITY', 'score': 0.9777434, 'word': ' Smalltown,', 'start': 530, 'end': 541}\n",
      "{'entity_group': 'EMAIL', 'score': 0.9984106, 'word': ' michael.johnson@example.com', 'start': 606, 'end': 634}\n",
      "{'entity_group': 'PHONE_NUMBER', 'score': 0.78792256, 'word': ' +1 (555) 222-3333.', 'start': 658, 'end': 677}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\Python311\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:392: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# lakshyakh93/deberta_finetuned_pii\n",
    "from transformers import pipeline\n",
    "gen = pipeline(\"token-classification\", \"lakshyakh93/deberta_finetuned_pii\", device=-1)\n",
    "output = gen(text, aggregation_strategy=\"first\")\n",
    "for data in output:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Johnson,\n",
      " alice.johnson@example.com.\n",
      " February 14, 1990,\n",
      " $7500\n",
      " 555-987-6543\n",
      " www.alicejohnson.com.\n",
      " Alice\n",
      " July 20, 2021,\n",
      " 10:00 AM.\n",
      " $1,000,000,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Softwares\\Python311\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:392: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# lakshyakh93/deberta_finetuned_pii\n",
    "from transformers import pipeline\n",
    "gen = pipeline(\"token-classification\", \"lakshyakh93/deberta_finetuned_pii\", device=-1)\n",
    "\n",
    "text = sample_text\n",
    "output = gen(text, aggregation_strategy=\"first\")\n",
    "for data in output:\n",
    "    print(data['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_args_parser',\n",
       " '_basic_tokenizer',\n",
       " '_batch_size',\n",
       " '_ensure_tensor_on_device',\n",
       " '_forward',\n",
       " '_forward_params',\n",
       " '_num_workers',\n",
       " '_postprocess_params',\n",
       " '_preprocess_params',\n",
       " '_sanitize_parameters',\n",
       " 'aggregate',\n",
       " 'aggregate_overlapping_entities',\n",
       " 'aggregate_word',\n",
       " 'aggregate_words',\n",
       " 'binary_output',\n",
       " 'call_count',\n",
       " 'check_model_type',\n",
       " 'default_input_names',\n",
       " 'device',\n",
       " 'device_placement',\n",
       " 'ensure_tensor_on_device',\n",
       " 'feature_extractor',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'gather_pre_entities',\n",
       " 'get_inference_context',\n",
       " 'get_iterator',\n",
       " 'get_tag',\n",
       " 'group_entities',\n",
       " 'group_sub_entities',\n",
       " 'image_processor',\n",
       " 'iterate',\n",
       " 'model',\n",
       " 'modelcard',\n",
       " 'postprocess',\n",
       " 'predict',\n",
       " 'preprocess',\n",
       " 'run_multi',\n",
       " 'run_single',\n",
       " 'save_pretrained',\n",
       " 'task',\n",
       " 'tokenizer',\n",
       " 'torch_dtype',\n",
       " 'transform']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "gen = pipeline(\"token-classification\", \"lakshyakh93/deberta_finetuned_pii\", device=-1)\n",
    "dir(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.token_classification.TokenClassificationPipeline at 0x23703eb3e90>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Types:\n",
      "bos_token\n",
      "eos_token\n",
      "unk_token\n",
      "sep_token\n",
      "pad_token\n",
      "cls_token\n",
      "mask_token\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaTokenizerFast\n",
    "\n",
    "# Load the DeBERTa tokenizer\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained(\"lakshyakh93/deberta_finetuned_pii\")\n",
    "\n",
    "# Get the special tokens\n",
    "special_tokens = tokenizer.special_tokens_map_extended\n",
    "\n",
    "# Filter out the entity types\n",
    "entity_types = [token for token, token_type in special_tokens.items()]\n",
    "\n",
    "# Print the entity types\n",
    "print(\"Entity Types:\")\n",
    "for entity_type in entity_types:\n",
    "    print(entity_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    [0, 5, 'John ', ' John'],\n",
    "    [4, 9, ' Doe,', ' Doe,'],\n",
    "    [17, 36, ' January 1st, 1980,', '[[DATE]]'],\n",
    "    [47, 64, ' 123 Main Street,', '[[STREETADDRESS]]'],\n",
    "    [64, 73, ' Anytown,', '[[STATE]]'],\n",
    "    [151, 172, ' john.doe@example.com', '[[EMAIL]]'],\n",
    "    [196, 215, ' +1 (555) 123-4567.', '[[PHONE_NUMBER]]']\n",
    "]\n",
    "\n",
    "[\n",
    "    [1, 5, 'ohn ', 'John'],\n",
    "    [5, 9, 'Doe,', 'Doe,'],\n",
    "    [18, 36, 'January 1st, 1980,', '[[DATE]]'],\n",
    "    [48, 64, '123 Main Street,', '[[STREETADDRESS]]'],\n",
    "    [65, 73, 'Anytown,', '[[STATE]]'],\n",
    "    [152, 172, 'john.doe@example.com', '[[EMAIL]]'],\n",
    "    [197, 215, '+1 (555) 123-4567.', '[[PHONE_NUMBER]]']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    [\n",
    "        '[[PERSON]], born on January 1st, 1980, resides at 123 Main Street, Anytown, USA. He works at XYZ Corporation as a software engineer. His email address is [[PERSON]] and his phone number is +1 (555) 123-4567. ',\n",
    "        []\n",
    "    ],\n",
    "    ['John Doe, born on [[DATE]] resides at [[STREETADDRESS]] [[STATE]] USA. He works at XYZ Corporation as a software engineer. His email address is [[EMAIL]] and his phone number is [[PHONE_NUMBER]] ',\n",
    "        [\n",
    "            [0, 4, 'John', 'John'],\n",
    "            [5, 9, 'Doe,', 'Doe,']\n",
    "        ]\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "front space =  True\n",
    "back space =  False\n",
    "original = ' John'\n",
    "entity = ' John'\n",
    "front space =  True\n",
    "back space =  False\n",
    "original = ' Doe,'\n",
    "entity = ' Doe,'\n",
    "front space =  True\n",
    "back space =  False\n",
    "original = ' January 1st, 1980,'\n",
    "entity = ' January 1st, 1980,'\n",
    "front space =  True\n",
    "back space =  False\n",
    "original = ' 123 Main Street,'\n",
    "entity = ' 123 Main Street,'\n",
    "front space =  True\n",
    "back space =  False\n",
    "original = ' Anytown,'\n",
    "entity = ' Anytown,'\n",
    "front space =  True\n",
    "back space =  False\n",
    "original = ' john.doe@example.com'\n",
    "entity = ' john.doe@example.com'\n",
    "front space =  True\n",
    "back space =  False\n",
    "original = ' +1 (555) 123-4567.'\n",
    "entity = ' +1 (555) 123-4567.'\n",
    "\n",
    "Replacement\n",
    "[\n",
    "    [1, 6, 'ohn D', ' John'],\n",
    "    [5, 10, 'Doe, ', ' Doe,'],\n",
    "    [18, 37, 'January 1st, 1980, ', '[[DATE]]'],\n",
    "    [48, 65, '123 Main Street, ', '[[STREETADDRESS]]'],\n",
    "    [65, 74, 'Anytown, ', '[[STATE]]'],\n",
    "    [152, 173, 'john.doe@example.com ', '[[EMAIL]]'],\n",
    "    [197, 216, '+1 (555) 123-4567. ', '[[PHONE_NUMBER]]']\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
